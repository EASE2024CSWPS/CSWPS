# ECML_PKDD2024_CSWPS
This is a replication package for Paper `Code Summarization with Project-Specific Features`.<br>


## Content
1. [Project Summary](#1-Project-Summary)<br>
2. [Get Started](#2-Get-Started)<br>
&ensp;&ensp;[2.1 Requirements](#21-Requirements)<br>
&ensp;&ensp;[2.2 Dataset](#22-Dataset)<br>
&ensp;&ensp;[2.3 Stage-1 Comment Classification](#Comment_Classification)<br>
&ensp;&ensp;[2.4 Stage-2 Comment Generator with Project-Specific](#Comment_Generator_with_Project-Specific)<br>

## 1 Project Summary
Code summarization aims to automatically generate natural language descriptions for code snippets, which help people maintain and understand code snippets. Existing code summarization methods are mostly based on the encoder-decoder structure, where the encoder learns latent features from a code snippet and the decoder generates the corresponding summary based on the features. Such methods do not leverage project-specific information and tend to generate general summaries. However, in practice developers want the generated summaries to be project-specific, i.e., being consistent with the existing summaries in the same project on aspects such as sentence patterns and domain concepts. In this work, we investigate project-specific code summarization. We propose a two-stage method CSWPS, which can be seamlessly integrated into any existing encoder-decoder summarization model. In the first stage, CSWPS learns project-specific features from existing summaries in each project using multi-task learning. In the second stage, CSWPS samples from the project-specific features conditioned on the input source code and project information, and extracts the features most relevant to the input code. The features guide the decoder to generate a project-specific summary for the input code. By incorporating CSWPS into existing code summarization models, we can always improve their performance and achieve the new state-of-the-art. We also empirically show that the summaries generated by incorporating CSWPS are more project-specific, via feature visualization and human study. 


## 2 Get Started
### 2.1 Requirements
* Hardwares: NVIDIA GeForce Tesla V100 GPU
* OS: Ubuntu 20.04
* Packages: 
  * python 3.9
  * pytorch 2.0.1
  * cuda 11.8
  * numpy
  * tqdm



### 2.2 Dataset
*CSWPS* is evaluated on [PCS](https://github.com/pkuserc/MPCos_ASE2022.git) and [CodeXGLUE](https://github.com/microsoft/CodeXGLUE) datasets.<br>

Since *CSWPS* can be used on any encoder-decoder structure code annotation generation model. We provide the original data in `raw_data` folder.<br>

For example, the CS model in this repository is based on SCRIPT. Therefore, if you want to reproduce the experimental results, you can refer to [SIT](https://github.com/gingasan/sit3) or [SCRIPT](https://github.com/GoneZ5/SCRIPT) and construct the corresponding structural data.

> The structure of data folder should be like below:

```
CodeXGLUE
├── dev
│   ├── code.guid
│   ├── code.original
│   ├── code.original_subtoken
│   ├── code.repo
│   └── javadoc.original
├── distance
│   ├── 41480.npy
│   ├── 41481.npy
│   ├── ...
├── structure
│   ├── 41480.npy
│   ├── 41481.npy
│   ├── ...
├── test
│   ├── code.guid
│   ├── code.original
│   ├── code.original_subtoken
│   ├── code.repo
│   └── javadoc.original
└── train
    ├── code.guid
    ├── code.original
    ├── code.original_subtoken
    ├── code.repo
    └── javadoc.original
```

### Comment_Classification

To run the classification model, you can go to the `stage_1_cls/scripts` folder and run the following cmd:

```
CUDA_VISIBLE_DEVICES=0 sh ./${MODEL}.sh
```

You can change `0` to any GPU device id you want, `${MODEL}` should be `CodeXGLUE` or `PCS`.

After training, the model will automatically generate a `style_info` folder in the project root directory, and store the `Latent Summary Representations` in the training set in different folders according to project classification for use in the second stage.

### Comment_Generator_with_Project-Specific

The version implemented in this repository is `SCRIPT+CSWPS`, so the running command is similar to `SCRIPT`.

To run the `CSWPS` model, you can go to the `stage_2_cls/scripts` folder and run the following cmd:

```
CUDA_VISIBLE_DEVICES=0 sh ./${MODEL}.sh
```

You can change `0` to any GPU device id you want, `${MODEL}` should be `CodeXGLUE` or `PCS`.

The script will be executed in the order of train, test, beam search, and the results will be saved locally.




